{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import modin.pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "import ray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-04 10:22:43.565854: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-06-04 10:22:43.631514: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-06-04 10:22:43.632785: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-06-04 10:22:45.129234: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_ranking as tfr\n",
    "from tensorflow_serving.apis import input_pb2\n",
    "from sklearn.datasets import load_svmlight_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UserWarning: When using a pre-initialized Ray cluster, please ensure that the runtime env sets environment variable __MODIN_AUTOIMPORT_PANDAS__ to 1\n"
     ]
    }
   ],
   "source": [
    "folder = 1\n",
    "\n",
    "df_train = pd.read_csv(f'MLSR/Fold{folder}/train.txt', sep=' ', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>129</th>\n",
       "      <th>130</th>\n",
       "      <th>131</th>\n",
       "      <th>132</th>\n",
       "      <th>133</th>\n",
       "      <th>134</th>\n",
       "      <th>135</th>\n",
       "      <th>136</th>\n",
       "      <th>137</th>\n",
       "      <th>138</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>qid:1</td>\n",
       "      <td>1:3</td>\n",
       "      <td>2:3</td>\n",
       "      <td>3:0</td>\n",
       "      <td>4:0</td>\n",
       "      <td>5:3</td>\n",
       "      <td>6:1</td>\n",
       "      <td>7:1</td>\n",
       "      <td>8:0</td>\n",
       "      <td>...</td>\n",
       "      <td>128:11089534</td>\n",
       "      <td>129:2</td>\n",
       "      <td>130:116</td>\n",
       "      <td>131:64034</td>\n",
       "      <td>132:13</td>\n",
       "      <td>133:3</td>\n",
       "      <td>134:0</td>\n",
       "      <td>135:0</td>\n",
       "      <td>136:0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>qid:1</td>\n",
       "      <td>1:3</td>\n",
       "      <td>2:0</td>\n",
       "      <td>3:3</td>\n",
       "      <td>4:0</td>\n",
       "      <td>5:3</td>\n",
       "      <td>6:1</td>\n",
       "      <td>7:0</td>\n",
       "      <td>8:1</td>\n",
       "      <td>...</td>\n",
       "      <td>128:11089534</td>\n",
       "      <td>129:2</td>\n",
       "      <td>130:124</td>\n",
       "      <td>131:64034</td>\n",
       "      <td>132:1</td>\n",
       "      <td>133:2</td>\n",
       "      <td>134:0</td>\n",
       "      <td>135:0</td>\n",
       "      <td>136:0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>qid:1</td>\n",
       "      <td>1:3</td>\n",
       "      <td>2:0</td>\n",
       "      <td>3:2</td>\n",
       "      <td>4:0</td>\n",
       "      <td>5:3</td>\n",
       "      <td>6:1</td>\n",
       "      <td>7:0</td>\n",
       "      <td>8:0.666667</td>\n",
       "      <td>...</td>\n",
       "      <td>128:3</td>\n",
       "      <td>129:1</td>\n",
       "      <td>130:124</td>\n",
       "      <td>131:3344</td>\n",
       "      <td>132:14</td>\n",
       "      <td>133:67</td>\n",
       "      <td>134:0</td>\n",
       "      <td>135:0</td>\n",
       "      <td>136:0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>qid:1</td>\n",
       "      <td>1:3</td>\n",
       "      <td>2:0</td>\n",
       "      <td>3:3</td>\n",
       "      <td>4:0</td>\n",
       "      <td>5:3</td>\n",
       "      <td>6:1</td>\n",
       "      <td>7:0</td>\n",
       "      <td>8:1</td>\n",
       "      <td>...</td>\n",
       "      <td>128:11089534</td>\n",
       "      <td>129:13</td>\n",
       "      <td>130:123</td>\n",
       "      <td>131:63933</td>\n",
       "      <td>132:1</td>\n",
       "      <td>133:3</td>\n",
       "      <td>134:0</td>\n",
       "      <td>135:0</td>\n",
       "      <td>136:0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>qid:1</td>\n",
       "      <td>1:3</td>\n",
       "      <td>2:0</td>\n",
       "      <td>3:3</td>\n",
       "      <td>4:0</td>\n",
       "      <td>5:3</td>\n",
       "      <td>6:1</td>\n",
       "      <td>7:0</td>\n",
       "      <td>8:1</td>\n",
       "      <td>...</td>\n",
       "      <td>128:5</td>\n",
       "      <td>129:7</td>\n",
       "      <td>130:256</td>\n",
       "      <td>131:49697</td>\n",
       "      <td>132:1</td>\n",
       "      <td>133:13</td>\n",
       "      <td>134:0</td>\n",
       "      <td>135:0</td>\n",
       "      <td>136:0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>qid:1</td>\n",
       "      <td>1:3</td>\n",
       "      <td>2:0</td>\n",
       "      <td>3:3</td>\n",
       "      <td>4:0</td>\n",
       "      <td>5:3</td>\n",
       "      <td>6:1</td>\n",
       "      <td>7:0</td>\n",
       "      <td>8:1</td>\n",
       "      <td>...</td>\n",
       "      <td>128:6</td>\n",
       "      <td>129:7</td>\n",
       "      <td>130:210</td>\n",
       "      <td>131:49923</td>\n",
       "      <td>132:5</td>\n",
       "      <td>133:15</td>\n",
       "      <td>134:0</td>\n",
       "      <td>135:0</td>\n",
       "      <td>136:0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>qid:1</td>\n",
       "      <td>1:3</td>\n",
       "      <td>2:0</td>\n",
       "      <td>3:3</td>\n",
       "      <td>4:0</td>\n",
       "      <td>5:3</td>\n",
       "      <td>6:1</td>\n",
       "      <td>7:0</td>\n",
       "      <td>8:1</td>\n",
       "      <td>...</td>\n",
       "      <td>128:6</td>\n",
       "      <td>129:7</td>\n",
       "      <td>130:256</td>\n",
       "      <td>131:50023</td>\n",
       "      <td>132:3</td>\n",
       "      <td>133:14</td>\n",
       "      <td>134:0</td>\n",
       "      <td>135:0</td>\n",
       "      <td>136:0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2</td>\n",
       "      <td>qid:1</td>\n",
       "      <td>1:3</td>\n",
       "      <td>2:0</td>\n",
       "      <td>3:3</td>\n",
       "      <td>4:0</td>\n",
       "      <td>5:3</td>\n",
       "      <td>6:1</td>\n",
       "      <td>7:0</td>\n",
       "      <td>8:1</td>\n",
       "      <td>...</td>\n",
       "      <td>128:0</td>\n",
       "      <td>129:17</td>\n",
       "      <td>130:115</td>\n",
       "      <td>131:63318</td>\n",
       "      <td>132:1</td>\n",
       "      <td>133:3</td>\n",
       "      <td>134:0</td>\n",
       "      <td>135:0</td>\n",
       "      <td>136:0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>qid:1</td>\n",
       "      <td>1:3</td>\n",
       "      <td>2:0</td>\n",
       "      <td>3:3</td>\n",
       "      <td>4:0</td>\n",
       "      <td>5:3</td>\n",
       "      <td>6:1</td>\n",
       "      <td>7:0</td>\n",
       "      <td>8:1</td>\n",
       "      <td>...</td>\n",
       "      <td>128:11089534</td>\n",
       "      <td>129:15</td>\n",
       "      <td>130:124</td>\n",
       "      <td>131:63363</td>\n",
       "      <td>132:1</td>\n",
       "      <td>133:3</td>\n",
       "      <td>134:0</td>\n",
       "      <td>135:0</td>\n",
       "      <td>136:0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>qid:1</td>\n",
       "      <td>1:3</td>\n",
       "      <td>2:0</td>\n",
       "      <td>3:3</td>\n",
       "      <td>4:0</td>\n",
       "      <td>5:3</td>\n",
       "      <td>6:1</td>\n",
       "      <td>7:0</td>\n",
       "      <td>8:1</td>\n",
       "      <td>...</td>\n",
       "      <td>128:1</td>\n",
       "      <td>129:1</td>\n",
       "      <td>130:541</td>\n",
       "      <td>131:63363</td>\n",
       "      <td>132:1</td>\n",
       "      <td>133:5</td>\n",
       "      <td>134:0</td>\n",
       "      <td>135:0</td>\n",
       "      <td>136:0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows x 139 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   0      1    2    3    4    5    6    7    8           9    ...  \\\n",
       "0    2  qid:1  1:3  2:3  3:0  4:0  5:3  6:1  7:1         8:0  ...   \n",
       "1    2  qid:1  1:3  2:0  3:3  4:0  5:3  6:1  7:0         8:1  ...   \n",
       "2    0  qid:1  1:3  2:0  3:2  4:0  5:3  6:1  7:0  8:0.666667  ...   \n",
       "3    2  qid:1  1:3  2:0  3:3  4:0  5:3  6:1  7:0         8:1  ...   \n",
       "4    1  qid:1  1:3  2:0  3:3  4:0  5:3  6:1  7:0         8:1  ...   \n",
       "5    1  qid:1  1:3  2:0  3:3  4:0  5:3  6:1  7:0         8:1  ...   \n",
       "6    1  qid:1  1:3  2:0  3:3  4:0  5:3  6:1  7:0         8:1  ...   \n",
       "7    2  qid:1  1:3  2:0  3:3  4:0  5:3  6:1  7:0         8:1  ...   \n",
       "8    1  qid:1  1:3  2:0  3:3  4:0  5:3  6:1  7:0         8:1  ...   \n",
       "9    0  qid:1  1:3  2:0  3:3  4:0  5:3  6:1  7:0         8:1  ...   \n",
       "\n",
       "            129     130      131        132     133     134    135    136  \\\n",
       "0  128:11089534   129:2  130:116  131:64034  132:13   133:3  134:0  135:0   \n",
       "1  128:11089534   129:2  130:124  131:64034   132:1   133:2  134:0  135:0   \n",
       "2         128:3   129:1  130:124   131:3344  132:14  133:67  134:0  135:0   \n",
       "3  128:11089534  129:13  130:123  131:63933   132:1   133:3  134:0  135:0   \n",
       "4         128:5   129:7  130:256  131:49697   132:1  133:13  134:0  135:0   \n",
       "5         128:6   129:7  130:210  131:49923   132:5  133:15  134:0  135:0   \n",
       "6         128:6   129:7  130:256  131:50023   132:3  133:14  134:0  135:0   \n",
       "7         128:0  129:17  130:115  131:63318   132:1   133:3  134:0  135:0   \n",
       "8  128:11089534  129:15  130:124  131:63363   132:1   133:3  134:0  135:0   \n",
       "9         128:1   129:1  130:541  131:63363   132:1   133:5  134:0  135:0   \n",
       "\n",
       "     137 138  \n",
       "0  136:0 NaN  \n",
       "1  136:0 NaN  \n",
       "2  136:0 NaN  \n",
       "3  136:0 NaN  \n",
       "4  136:0 NaN  \n",
       "5  136:0 NaN  \n",
       "6  136:0 NaN  \n",
       "7  136:0 NaN  \n",
       "8  136:0 NaN  \n",
       "9  136:0 NaN  \n",
       "\n",
       "[10 rows x 139 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Format the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_features():\n",
    "    '''This function processes and creates our feature columns descriptions'''\n",
    "    # Read in the features file\n",
    "    features = pd.read_csv('MLSR/features.csv')\n",
    "    # Create new header and replace spaces with underscore\n",
    "    new_header = features.iloc[0].str.replace(' ','_')\n",
    "    # Remove the first row which is now the new header\n",
    "    features = features[1:]\n",
    "    # Set new headers\n",
    "    features.columns = new_header\n",
    "    # Only the first cell for each category is filled. Using forward will\n",
    "    # will allow me to map each category to their sub-categories located\n",
    "    # in the stream column \n",
    "    features['feature_description'] = features['feature_description'].ffill()\n",
    "    # Replacing characters to allign with TensorFlows regex requirements\n",
    "    character_removal = [' ', '(', ')', '*']\n",
    "    for char in character_removal:\n",
    "        features['feature_description'] = features['feature_description'].str.replace(char, '_')\n",
    "        features['stream'] = features['stream'].astype(str).str.replace(char, '_')\n",
    "    # Setting column type to string for mapping within the load_rename_save function\n",
    "    features['feature_id'] = features['feature_id'].astype(str)\n",
    "    # Creating new column to map features to existing dataset\n",
    "    features['cols'] = 'string'\n",
    "    # Looping over all features and creating new column name\n",
    "    for idx in range(len(features)):\n",
    "        if str(features.iloc[idx]['stream']) != 'nan':\n",
    "            features['cols'].iloc[idx] = features['feature_description'].iloc[idx] + '_' + features['stream'].iloc[idx]\n",
    "        else:\n",
    "            features['cols'].iloc[idx] = features['feature_description'].iloc[idx]\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_columns(df):\n",
    "    '''This function labels the columns by descriptions\n",
    "       found on the microsoft research page'''    \n",
    "        \n",
    "    for col in df.columns:\n",
    "        if col == 0:\n",
    "            df.rename({col : 'relevance_label'}, axis=1, inplace=True)\n",
    "        elif col == 1:\n",
    "            df.rename({col : 'query_id'}, axis=1, inplace=True)\n",
    "        else:\n",
    "            df.rename({col : f'feature_{col - 1}'}, axis=1, inplace=True)\n",
    "            \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_rename_save(folder_num):\n",
    "    '''This function reads in all data located in folder n,\n",
    "       labels the columns, removes uneeded elements from the cells (i.e. 'qid:1' the qid is uneeded),\n",
    "       and saves the files as a parquet within folder n'''\n",
    "    \n",
    "    for folder in range(1, folder_num+1):\n",
    "        # Load data\n",
    "        df_train = pd.read_csv(f'MLSR/Fold{folder}/train.txt', sep=' ', header=None)\n",
    "        df_test = pd.read_csv(f'MLSR/Fold{folder}/test.txt', sep=' ', header=None)\n",
    "        df_val = pd.read_csv(f'MLSR/Fold{folder}/vali.txt', sep=' ', header=None)\n",
    "        \n",
    "        # Label the columns\n",
    "        df_train = label_columns(df_train)\n",
    "        df_test = label_columns(df_test)\n",
    "        df_val = label_columns(df_val)\n",
    "        \n",
    "        # Remove 'n:' from each column. The dataset assigned each feature number\n",
    "        # to the cells value which needs to be removed to get the data into int/float format\n",
    "        dataframes = {'train': df_train, 'test': df_test, 'val': df_val}\n",
    "        for k, df in dataframes.items():\n",
    "            for i in range(1,len(df.columns)-1):\n",
    "                df[f'feature_{i}'].replace(f'{i}:', '', regex=True, inplace=True)          \n",
    "            \n",
    "        # Only query_id was different than all of the other columns when assigning \n",
    "        # the prefix to the values. Here we remove 'qid:' from each cell\n",
    "            df['query_id'].replace('qid:', '', regex=True, inplace=True)\n",
    "\n",
    "        # Rename the feature columns from the given descriptions on Microsofts webiste   \n",
    "        features = preprocess_features()\n",
    "        \n",
    "        for k, df in dataframes.items():\n",
    "            for idx in range(len(features)):\n",
    "                id_ = features.iloc[idx]['feature_id']\n",
    "                for col in df.columns:\n",
    "                    if str(id_) == col.lstrip('feature_'):\n",
    "                        df.rename({col: features.iloc[idx]['cols']}, axis=1, inplace=True)\n",
    "        \n",
    "        # Save the cleaned dataset as a csv\n",
    "        df_train.to_csv(f'MLSR/Fold{folder}/df_train.csv', index=False)\n",
    "        df_test.to_csv(f'MLSR/Fold{folder}/df_test.csv', index=False)\n",
    "        df_val.to_csv(f'MLSR/Fold{folder}/df_val.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_rename_save(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query_id</th>\n",
       "      <th>covered_query_term_number_body</th>\n",
       "      <th>covered_query_term_number_anchor</th>\n",
       "      <th>covered_query_term_number_title</th>\n",
       "      <th>covered_query_term_number_url</th>\n",
       "      <th>covered_query_term_number_whole_document</th>\n",
       "      <th>covered_query_term_ratio_body</th>\n",
       "      <th>covered_query_term_ratio_anchor</th>\n",
       "      <th>covered_query_term_ratio_title</th>\n",
       "      <th>covered_query_term_ratio_url</th>\n",
       "      <th>...</th>\n",
       "      <th>Inlink_number</th>\n",
       "      <th>Outlink_number</th>\n",
       "      <th>PageRank</th>\n",
       "      <th>SiteRank</th>\n",
       "      <th>QualityScore</th>\n",
       "      <th>QualityScore2</th>\n",
       "      <th>Query-url_click_count</th>\n",
       "      <th>url_click_count</th>\n",
       "      <th>url_dwell_time</th>\n",
       "      <th>feature_137</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>relevance_label</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>11089534</td>\n",
       "      <td>2</td>\n",
       "      <td>116</td>\n",
       "      <td>64034</td>\n",
       "      <td>13</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>11089534</td>\n",
       "      <td>2</td>\n",
       "      <td>124</td>\n",
       "      <td>64034</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>124</td>\n",
       "      <td>3344</td>\n",
       "      <td>14</td>\n",
       "      <td>67</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>11089534</td>\n",
       "      <td>13</td>\n",
       "      <td>123</td>\n",
       "      <td>63933</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>256</td>\n",
       "      <td>49697</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>29992</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.5</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>13829</td>\n",
       "      <td>35302</td>\n",
       "      <td>21</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>29992</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.5</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>26074</td>\n",
       "      <td>35101</td>\n",
       "      <td>14</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>29992</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>11</td>\n",
       "      <td>2</td>\n",
       "      <td>2995</td>\n",
       "      <td>62170</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>29992</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>138</td>\n",
       "      <td>56419</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>29992</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>131</td>\n",
       "      <td>0</td>\n",
       "      <td>13556</td>\n",
       "      <td>25675</td>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>723412 rows x 138 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 query_id  covered_query_term_number_body  \\\n",
       "relevance_label                                             \n",
       "2                       1                               3   \n",
       "2                       1                               3   \n",
       "0                       1                               3   \n",
       "2                       1                               3   \n",
       "1                       1                               3   \n",
       "...                   ...                             ...   \n",
       "0                   29992                               2   \n",
       "0                   29992                               2   \n",
       "0                   29992                               2   \n",
       "0                   29992                               2   \n",
       "1                   29992                               2   \n",
       "\n",
       "                 covered_query_term_number_anchor  \\\n",
       "relevance_label                                     \n",
       "2                                               3   \n",
       "2                                               0   \n",
       "0                                               0   \n",
       "2                                               0   \n",
       "1                                               0   \n",
       "...                                           ...   \n",
       "0                                               0   \n",
       "0                                               1   \n",
       "0                                               2   \n",
       "0                                               0   \n",
       "1                                               1   \n",
       "\n",
       "                 covered_query_term_number_title  \\\n",
       "relevance_label                                    \n",
       "2                                              0   \n",
       "2                                              3   \n",
       "0                                              2   \n",
       "2                                              3   \n",
       "1                                              3   \n",
       "...                                          ...   \n",
       "0                                              1   \n",
       "0                                              1   \n",
       "0                                              2   \n",
       "0                                              0   \n",
       "1                                              1   \n",
       "\n",
       "                 covered_query_term_number_url  \\\n",
       "relevance_label                                  \n",
       "2                                            0   \n",
       "2                                            0   \n",
       "0                                            0   \n",
       "2                                            0   \n",
       "1                                            0   \n",
       "...                                        ...   \n",
       "0                                            1   \n",
       "0                                            1   \n",
       "0                                            2   \n",
       "0                                            0   \n",
       "1                                            0   \n",
       "\n",
       "                 covered_query_term_number_whole_document  \\\n",
       "relevance_label                                             \n",
       "2                                                       3   \n",
       "2                                                       3   \n",
       "0                                                       3   \n",
       "2                                                       3   \n",
       "1                                                       3   \n",
       "...                                                   ...   \n",
       "0                                                       2   \n",
       "0                                                       2   \n",
       "0                                                       2   \n",
       "0                                                       2   \n",
       "1                                                       2   \n",
       "\n",
       "                 covered_query_term_ratio_body  \\\n",
       "relevance_label                                  \n",
       "2                                          1.0   \n",
       "2                                          1.0   \n",
       "0                                          1.0   \n",
       "2                                          1.0   \n",
       "1                                          1.0   \n",
       "...                                        ...   \n",
       "0                                          1.0   \n",
       "0                                          1.0   \n",
       "0                                          1.0   \n",
       "0                                          1.0   \n",
       "1                                          1.0   \n",
       "\n",
       "                 covered_query_term_ratio_anchor  \\\n",
       "relevance_label                                    \n",
       "2                                            1.0   \n",
       "2                                            0.0   \n",
       "0                                            0.0   \n",
       "2                                            0.0   \n",
       "1                                            0.0   \n",
       "...                                          ...   \n",
       "0                                            0.0   \n",
       "0                                            0.5   \n",
       "0                                            1.0   \n",
       "0                                            0.0   \n",
       "1                                            0.5   \n",
       "\n",
       "                 covered_query_term_ratio_title  covered_query_term_ratio_url  \\\n",
       "relevance_label                                                                 \n",
       "2                                      0.000000                           0.0   \n",
       "2                                      1.000000                           0.0   \n",
       "0                                      0.666667                           0.0   \n",
       "2                                      1.000000                           0.0   \n",
       "1                                      1.000000                           0.0   \n",
       "...                                         ...                           ...   \n",
       "0                                      0.500000                           0.5   \n",
       "0                                      0.500000                           0.5   \n",
       "0                                      1.000000                           1.0   \n",
       "0                                      0.000000                           0.0   \n",
       "1                                      0.500000                           0.0   \n",
       "\n",
       "                 ...  Inlink_number  Outlink_number  PageRank  SiteRank  \\\n",
       "relevance_label  ...                                                      \n",
       "2                ...       11089534               2       116     64034   \n",
       "2                ...       11089534               2       124     64034   \n",
       "0                ...              3               1       124      3344   \n",
       "2                ...       11089534              13       123     63933   \n",
       "1                ...              5               7       256     49697   \n",
       "...              ...            ...             ...       ...       ...   \n",
       "0                ...              7               2     13829     35302   \n",
       "0                ...              0               0     26074     35101   \n",
       "0                ...             11               2      2995     62170   \n",
       "0                ...              0               0       138     56419   \n",
       "1                ...            131               0     13556     25675   \n",
       "\n",
       "                 QualityScore  QualityScore2  Query-url_click_count  \\\n",
       "relevance_label                                                       \n",
       "2                          13              3                      0   \n",
       "2                           1              2                      0   \n",
       "0                          14             67                      0   \n",
       "2                           1              3                      0   \n",
       "1                           1             13                      0   \n",
       "...                       ...            ...                    ...   \n",
       "0                          21              8                      0   \n",
       "0                          14              7                      1   \n",
       "0                           4              8                      0   \n",
       "0                           4              3                      0   \n",
       "1                           2             12                      0   \n",
       "\n",
       "                 url_click_count  url_dwell_time  feature_137  \n",
       "relevance_label                                                \n",
       "2                              0             0.0          NaN  \n",
       "2                              0             0.0          NaN  \n",
       "0                              0             0.0          NaN  \n",
       "2                              0             0.0          NaN  \n",
       "1                              0             0.0          NaN  \n",
       "...                          ...             ...          ...  \n",
       "0                              0             0.0          NaN  \n",
       "0                              0             0.0          NaN  \n",
       "0                              0             0.0          NaN  \n",
       "0                              0             0.0          NaN  \n",
       "1                              0             0.0          NaN  \n",
       "\n",
       "[723412 rows x 138 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tmp = pd.read_csv(f'MLSR/Fold1/df_train.csv', index_col=0)\n",
    "display(tmp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verify Data Integrity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_stats(folder_num):\n",
    "    ''' This function is to collect basic stats from the dataset. '''\n",
    "    for folder in range(1, folder_num+1):\n",
    "        # Load the data\n",
    "        df_train = pd.read_csv(f'MLSR/Fold{folder}/df_train.csv')\n",
    "        df_test = pd.read_csv(f'MLSR/Fold{folder}/df_test.csv')\n",
    "        df_val = pd.read_csv(f'MLSR/Fold{folder}/df_val.csv')\n",
    "        \n",
    "        # Collect metrics for below stats\n",
    "        len_train = len(df_train)\n",
    "        len_test = len(df_test)\n",
    "        len_val = len(df_val)\n",
    "        total = len_train + len_test + len_val\n",
    "        \n",
    "        # Print length of all datasets and the overal balance between the splits\n",
    "        print('*'*24 + ' ' + f'Folder Number {folder}' + ' ' + '*'*24)\n",
    "        print(f'Total rows in training set {folder}: {len_train}')\n",
    "        print(f'Total rows in testing set {folder}: {len_test}')\n",
    "        print(f'Total rows in validation set {folder}: {len_val}')\n",
    "        print('='*64)\n",
    "        print(f'The training set contains {round(len_train/total, 2) * 100}% of the total data')\n",
    "        print(f'The testing set contains {round(len_test/total, 2) * 100}% of the total data')\n",
    "        print(f'The validation set contains {round(len_val/total, 2) * 100}% of the total data')\n",
    "        print('='*64)\n",
    "        \n",
    "        # Create new dataframe showing NaN values\n",
    "        df_train_ = pd.DataFrame(df_train.isna().sum(), columns=['NaN_values'])\n",
    "        df_test_ = pd.DataFrame(df_test.isna().sum(), columns=['NaN_values'])\n",
    "        df_val_ = pd.DataFrame(df_val.isna().sum(), columns=['NaN_values'])\n",
    "        \n",
    "        # Mapping of NaN dataframes\n",
    "        nan_dataframes = {'df_train':df_train_, 'df_test':df_test_, 'df_val':df_val_}\n",
    "        \n",
    "        # Print the total percentage of missing values per column in each dataframe\n",
    "        for k,df in nan_dataframes.items():\n",
    "            df = df[df['NaN_values'] > 0]\n",
    "            total_missing = [len(df.index)]\n",
    "            for missing in total_missing:\n",
    "                if k == 'df_train':\n",
    "                    print(f'df_train_ Column {df.index[missing - 1]} is missing {df.values[missing - 1] / len(df_train) * 100}% of its data')\n",
    "                elif k == 'df_test':\n",
    "                    print(f'df_test_ Column {df.index[missing - 1]} is missing {df.values[missing - 1] / len(df_test) * 100}% of its data')\n",
    "                else:\n",
    "                    print(f'df_val_ Column {df.index[missing - 1]} is missing {df.values[missing - 1] / len(df_val) * 100}% of its data')\n",
    "        \n",
    "        # Mapping of initial dataframes\n",
    "        dataframes = {'df_train': df_train, 'df_test': df_test, 'df_val': df_val}\n",
    "        \n",
    "        # Calculating the distribution of the relevance column\n",
    "        for k,df in dataframes.items():\n",
    "            df_len = len(df)\n",
    "            relevance_counts = df['relevance_label'].value_counts()\n",
    "            print('='*64)\n",
    "            print('*'*16 + ' ' + f'{k} Relevance Class Balance' + ' ' + '*'*16)\n",
    "            for i in [0,1,2,3,4]:\n",
    "                print(f'Rank {i}: Total Count: {relevance_counts[i]} Percentage: {round(relevance_counts[i]/df_len,2) * 100}%')\n",
    "        print(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************************ Folder Number 1 ************************\n",
      "Total rows in training set 1: 723412\n",
      "Total rows in testing set 1: 241521\n",
      "Total rows in validation set 1: 235259\n",
      "================================================================\n",
      "The training set contains 60.0% of the total data\n",
      "The testing set contains 20.0% of the total data\n",
      "The validation set contains 20.0% of the total data\n",
      "================================================================\n",
      "df_train_ Column feature_137 is missing [100.]% of its data\n",
      "df_test_ Column feature_137 is missing [100.]% of its data\n",
      "df_val_ Column feature_137 is missing [100.]% of its data\n",
      "================================================================\n",
      "**************** df_train Relevance Class Balance ****************\n",
      "Rank 0: Total Count: 377957 Percentage: 52.0%\n",
      "Rank 1: Total Count: 232569 Percentage: 32.0%\n",
      "Rank 2: Total Count: 95082 Percentage: 13.0%\n",
      "Rank 3: Total Count: 12658 Percentage: 2.0%\n",
      "Rank 4: Total Count: 5146 Percentage: 1.0%\n",
      "================================================================\n",
      "**************** df_test Relevance Class Balance ****************\n",
      "Rank 0: Total Count: 124784 Percentage: 52.0%\n",
      "Rank 1: Total Count: 77896 Percentage: 32.0%\n",
      "Rank 2: Total Count: 32459 Percentage: 13.0%\n",
      "Rank 3: Total Count: 4450 Percentage: 2.0%\n",
      "Rank 4: Total Count: 1932 Percentage: 1.0%\n",
      "================================================================\n",
      "**************** df_val Relevance Class Balance ****************\n",
      "Rank 0: Total Count: 121522 Percentage: 52.0%\n",
      "Rank 1: Total Count: 75815 Percentage: 32.0%\n",
      "Rank 2: Total Count: 31910 Percentage: 14.000000000000002%\n",
      "Rank 3: Total Count: 4209 Percentage: 2.0%\n",
      "Rank 4: Total Count: 1803 Percentage: 1.0%\n",
      " \n",
      "************************ Folder Number 2 ************************\n",
      "Total rows in training set 2: 716683\n",
      "Total rows in testing set 2: 241988\n",
      "Total rows in validation set 2: 241521\n",
      "================================================================\n",
      "The training set contains 60.0% of the total data\n",
      "The testing set contains 20.0% of the total data\n",
      "The validation set contains 20.0% of the total data\n",
      "================================================================\n",
      "df_train_ Column feature_137 is missing [100.]% of its data\n",
      "df_test_ Column feature_137 is missing [100.]% of its data\n",
      "df_val_ Column feature_137 is missing [100.]% of its data\n",
      "================================================================\n",
      "**************** df_train Relevance Class Balance ****************\n",
      "Rank 0: Total Count: 373029 Percentage: 52.0%\n",
      "Rank 1: Total Count: 230368 Percentage: 32.0%\n",
      "Rank 2: Total Count: 95117 Percentage: 13.0%\n",
      "Rank 3: Total Count: 12814 Percentage: 2.0%\n",
      "Rank 4: Total Count: 5355 Percentage: 1.0%\n",
      "================================================================\n",
      "**************** df_test Relevance Class Balance ****************\n",
      "Rank 0: Total Count: 126450 Percentage: 52.0%\n",
      "Rank 1: Total Count: 78016 Percentage: 32.0%\n",
      "Rank 2: Total Count: 31875 Percentage: 13.0%\n",
      "Rank 3: Total Count: 4053 Percentage: 2.0%\n",
      "Rank 4: Total Count: 1594 Percentage: 1.0%\n",
      "================================================================\n",
      "**************** df_val Relevance Class Balance ****************\n",
      "Rank 0: Total Count: 124784 Percentage: 52.0%\n",
      "Rank 1: Total Count: 77896 Percentage: 32.0%\n",
      "Rank 2: Total Count: 32459 Percentage: 13.0%\n",
      "Rank 3: Total Count: 4450 Percentage: 2.0%\n",
      "Rank 4: Total Count: 1932 Percentage: 1.0%\n",
      " \n",
      "************************ Folder Number 3 ************************\n",
      "Total rows in training set 3: 719111\n",
      "Total rows in testing set 3: 239093\n",
      "Total rows in validation set 3: 241988\n",
      "================================================================\n",
      "The training set contains 60.0% of the total data\n",
      "The testing set contains 20.0% of the total data\n",
      "The validation set contains 20.0% of the total data\n",
      "================================================================\n",
      "df_train_ Column feature_137 is missing [100.]% of its data\n",
      "df_test_ Column feature_137 is missing [100.]% of its data\n",
      "df_val_ Column feature_137 is missing [100.]% of its data\n",
      "================================================================\n",
      "**************** df_train Relevance Class Balance ****************\n",
      "Rank 0: Total Count: 371725 Percentage: 52.0%\n",
      "Rank 1: Total Count: 232302 Percentage: 32.0%\n",
      "Rank 2: Total Count: 96663 Percentage: 13.0%\n",
      "Rank 3: Total Count: 12903 Percentage: 2.0%\n",
      "Rank 4: Total Count: 5518 Percentage: 1.0%\n",
      "================================================================\n",
      "**************** df_test Relevance Class Balance ****************\n",
      "Rank 0: Total Count: 126088 Percentage: 53.0%\n",
      "Rank 1: Total Count: 75962 Percentage: 32.0%\n",
      "Rank 2: Total Count: 30913 Percentage: 13.0%\n",
      "Rank 3: Total Count: 4361 Percentage: 2.0%\n",
      "Rank 4: Total Count: 1769 Percentage: 1.0%\n",
      "================================================================\n",
      "**************** df_val Relevance Class Balance ****************\n",
      "Rank 0: Total Count: 126450 Percentage: 52.0%\n",
      "Rank 1: Total Count: 78016 Percentage: 32.0%\n",
      "Rank 2: Total Count: 31875 Percentage: 13.0%\n",
      "Rank 3: Total Count: 4053 Percentage: 2.0%\n",
      "Rank 4: Total Count: 1594 Percentage: 1.0%\n",
      " \n",
      "************************ Folder Number 4 ************************\n",
      "Total rows in training set 4: 718768\n",
      "Total rows in testing set 4: 242331\n",
      "Total rows in validation set 4: 239093\n",
      "================================================================\n",
      "The training set contains 60.0% of the total data\n",
      "The testing set contains 20.0% of the total data\n",
      "The validation set contains 20.0% of the total data\n",
      "================================================================\n",
      "df_train_ Column feature_137 is missing [100.]% of its data\n",
      "df_test_ Column feature_137 is missing [100.]% of its data\n",
      "df_val_ Column feature_137 is missing [100.]% of its data\n",
      "================================================================\n",
      "**************** df_train Relevance Class Balance ****************\n",
      "Rank 0: Total Count: 372756 Percentage: 52.0%\n",
      "Rank 1: Total Count: 231727 Percentage: 32.0%\n",
      "Rank 2: Total Count: 96244 Percentage: 13.0%\n",
      "Rank 3: Total Count: 12712 Percentage: 2.0%\n",
      "Rank 4: Total Count: 5329 Percentage: 1.0%\n",
      "================================================================\n",
      "**************** df_test Relevance Class Balance ****************\n",
      "Rank 0: Total Count: 125419 Percentage: 52.0%\n",
      "Rank 1: Total Count: 78591 Percentage: 32.0%\n",
      "Rank 2: Total Count: 32294 Percentage: 13.0%\n",
      "Rank 3: Total Count: 4244 Percentage: 2.0%\n",
      "Rank 4: Total Count: 1783 Percentage: 1.0%\n",
      "================================================================\n",
      "**************** df_val Relevance Class Balance ****************\n",
      "Rank 0: Total Count: 126088 Percentage: 53.0%\n",
      "Rank 1: Total Count: 75962 Percentage: 32.0%\n",
      "Rank 2: Total Count: 30913 Percentage: 13.0%\n",
      "Rank 3: Total Count: 4361 Percentage: 2.0%\n",
      "Rank 4: Total Count: 1769 Percentage: 1.0%\n",
      " \n",
      "************************ Folder Number 5 ************************\n",
      "Total rows in training set 5: 722602\n",
      "Total rows in testing set 5: 235259\n",
      "Total rows in validation set 5: 242331\n",
      "================================================================\n",
      "The training set contains 60.0% of the total data\n",
      "The testing set contains 20.0% of the total data\n",
      "The validation set contains 20.0% of the total data\n",
      "================================================================\n",
      "df_train_ Column feature_137 is missing [100.]% of its data\n",
      "df_test_ Column feature_137 is missing [100.]% of its data\n",
      "df_val_ Column feature_137 is missing [100.]% of its data\n",
      "================================================================\n",
      "**************** df_train Relevance Class Balance ****************\n",
      "Rank 0: Total Count: 377322 Percentage: 52.0%\n",
      "Rank 1: Total Count: 231874 Percentage: 32.0%\n",
      "Rank 2: Total Count: 95247 Percentage: 13.0%\n",
      "Rank 3: Total Count: 12864 Percentage: 2.0%\n",
      "Rank 4: Total Count: 5295 Percentage: 1.0%\n",
      "================================================================\n",
      "**************** df_test Relevance Class Balance ****************\n",
      "Rank 0: Total Count: 121522 Percentage: 52.0%\n",
      "Rank 1: Total Count: 75815 Percentage: 32.0%\n",
      "Rank 2: Total Count: 31910 Percentage: 14.000000000000002%\n",
      "Rank 3: Total Count: 4209 Percentage: 2.0%\n",
      "Rank 4: Total Count: 1803 Percentage: 1.0%\n",
      "================================================================\n",
      "**************** df_val Relevance Class Balance ****************\n",
      "Rank 0: Total Count: 125419 Percentage: 52.0%\n",
      "Rank 1: Total Count: 78591 Percentage: 32.0%\n",
      "Rank 2: Total Count: 32294 Percentage: 13.0%\n",
      "Rank 3: Total Count: 4244 Percentage: 2.0%\n",
      "Rank 4: Total Count: 1783 Percentage: 1.0%\n",
      " \n"
     ]
    }
   ],
   "source": [
    "data_stats(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_unwanted_cols(folder_num):\n",
    "    '''This function drops column 137 from each dataframe due to 137 missing \n",
    "        100% of its values across all datasets'''\n",
    "    for folder in range(1, folder_num+1):\n",
    "        df_train = pd.read_csv(f'MLSR/Fold{folder}/df_train.csv')\n",
    "        df_test = pd.read_csv(f'MLSR/Fold{folder}/df_test.csv')\n",
    "        df_val = pd.read_csv(f'MLSR/Fold{folder}/df_val.csv')\n",
    "        \n",
    "        df_train.drop('feature_137', axis=1, inplace=True)\n",
    "        df_test.drop('feature_137', axis=1, inplace=True)\n",
    "        df_val.drop('feature_137', axis=1, inplace=True)\n",
    "        \n",
    "        df_train.to_csv(f'MLSR/Fold{folder}/df_train.csv', index=False)\n",
    "        df_test.to_csv(f'MLSR/Fold{folder}/df_test.csv', index=False)\n",
    "        df_val.to_csv(f'MLSR/Fold{folder}/df_val.csv', index=False)\n",
    "        print(f'Finished Cleaning Fold{folder}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Cleaning Fold1\n",
      "Finished Cleaning Fold2\n",
      "Finished Cleaning Fold3\n",
      "Finished Cleaning Fold4\n",
      "Finished Cleaning Fold5\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query_id</th>\n",
       "      <th>covered_query_term_number_body</th>\n",
       "      <th>covered_query_term_number_anchor</th>\n",
       "      <th>covered_query_term_number_title</th>\n",
       "      <th>covered_query_term_number_url</th>\n",
       "      <th>covered_query_term_number_whole_document</th>\n",
       "      <th>covered_query_term_ratio_body</th>\n",
       "      <th>covered_query_term_ratio_anchor</th>\n",
       "      <th>covered_query_term_ratio_title</th>\n",
       "      <th>covered_query_term_ratio_url</th>\n",
       "      <th>...</th>\n",
       "      <th>Length_of_URL</th>\n",
       "      <th>Inlink_number</th>\n",
       "      <th>Outlink_number</th>\n",
       "      <th>PageRank</th>\n",
       "      <th>SiteRank</th>\n",
       "      <th>QualityScore</th>\n",
       "      <th>QualityScore2</th>\n",
       "      <th>Query-url_click_count</th>\n",
       "      <th>url_click_count</th>\n",
       "      <th>url_dwell_time</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>relevance_label</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>62</td>\n",
       "      <td>11089534</td>\n",
       "      <td>2</td>\n",
       "      <td>116</td>\n",
       "      <td>64034</td>\n",
       "      <td>13</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>54</td>\n",
       "      <td>11089534</td>\n",
       "      <td>2</td>\n",
       "      <td>124</td>\n",
       "      <td>64034</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>45</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>124</td>\n",
       "      <td>3344</td>\n",
       "      <td>14</td>\n",
       "      <td>67</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>56</td>\n",
       "      <td>11089534</td>\n",
       "      <td>13</td>\n",
       "      <td>123</td>\n",
       "      <td>63933</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>64</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>256</td>\n",
       "      <td>49697</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>29992</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.5</td>\n",
       "      <td>...</td>\n",
       "      <td>39</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>13829</td>\n",
       "      <td>35302</td>\n",
       "      <td>21</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>29992</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.5</td>\n",
       "      <td>...</td>\n",
       "      <td>28</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>26074</td>\n",
       "      <td>35101</td>\n",
       "      <td>14</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>29992</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>58</td>\n",
       "      <td>11</td>\n",
       "      <td>2</td>\n",
       "      <td>2995</td>\n",
       "      <td>62170</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>29992</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>65</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>138</td>\n",
       "      <td>56419</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>29992</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>30</td>\n",
       "      <td>131</td>\n",
       "      <td>0</td>\n",
       "      <td>13556</td>\n",
       "      <td>25675</td>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>723412 rows x 137 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 query_id  covered_query_term_number_body  \\\n",
       "relevance_label                                             \n",
       "2                       1                               3   \n",
       "2                       1                               3   \n",
       "0                       1                               3   \n",
       "2                       1                               3   \n",
       "1                       1                               3   \n",
       "...                   ...                             ...   \n",
       "0                   29992                               2   \n",
       "0                   29992                               2   \n",
       "0                   29992                               2   \n",
       "0                   29992                               2   \n",
       "1                   29992                               2   \n",
       "\n",
       "                 covered_query_term_number_anchor  \\\n",
       "relevance_label                                     \n",
       "2                                               3   \n",
       "2                                               0   \n",
       "0                                               0   \n",
       "2                                               0   \n",
       "1                                               0   \n",
       "...                                           ...   \n",
       "0                                               0   \n",
       "0                                               1   \n",
       "0                                               2   \n",
       "0                                               0   \n",
       "1                                               1   \n",
       "\n",
       "                 covered_query_term_number_title  \\\n",
       "relevance_label                                    \n",
       "2                                              0   \n",
       "2                                              3   \n",
       "0                                              2   \n",
       "2                                              3   \n",
       "1                                              3   \n",
       "...                                          ...   \n",
       "0                                              1   \n",
       "0                                              1   \n",
       "0                                              2   \n",
       "0                                              0   \n",
       "1                                              1   \n",
       "\n",
       "                 covered_query_term_number_url  \\\n",
       "relevance_label                                  \n",
       "2                                            0   \n",
       "2                                            0   \n",
       "0                                            0   \n",
       "2                                            0   \n",
       "1                                            0   \n",
       "...                                        ...   \n",
       "0                                            1   \n",
       "0                                            1   \n",
       "0                                            2   \n",
       "0                                            0   \n",
       "1                                            0   \n",
       "\n",
       "                 covered_query_term_number_whole_document  \\\n",
       "relevance_label                                             \n",
       "2                                                       3   \n",
       "2                                                       3   \n",
       "0                                                       3   \n",
       "2                                                       3   \n",
       "1                                                       3   \n",
       "...                                                   ...   \n",
       "0                                                       2   \n",
       "0                                                       2   \n",
       "0                                                       2   \n",
       "0                                                       2   \n",
       "1                                                       2   \n",
       "\n",
       "                 covered_query_term_ratio_body  \\\n",
       "relevance_label                                  \n",
       "2                                          1.0   \n",
       "2                                          1.0   \n",
       "0                                          1.0   \n",
       "2                                          1.0   \n",
       "1                                          1.0   \n",
       "...                                        ...   \n",
       "0                                          1.0   \n",
       "0                                          1.0   \n",
       "0                                          1.0   \n",
       "0                                          1.0   \n",
       "1                                          1.0   \n",
       "\n",
       "                 covered_query_term_ratio_anchor  \\\n",
       "relevance_label                                    \n",
       "2                                            1.0   \n",
       "2                                            0.0   \n",
       "0                                            0.0   \n",
       "2                                            0.0   \n",
       "1                                            0.0   \n",
       "...                                          ...   \n",
       "0                                            0.0   \n",
       "0                                            0.5   \n",
       "0                                            1.0   \n",
       "0                                            0.0   \n",
       "1                                            0.5   \n",
       "\n",
       "                 covered_query_term_ratio_title  covered_query_term_ratio_url  \\\n",
       "relevance_label                                                                 \n",
       "2                                      0.000000                           0.0   \n",
       "2                                      1.000000                           0.0   \n",
       "0                                      0.666667                           0.0   \n",
       "2                                      1.000000                           0.0   \n",
       "1                                      1.000000                           0.0   \n",
       "...                                         ...                           ...   \n",
       "0                                      0.500000                           0.5   \n",
       "0                                      0.500000                           0.5   \n",
       "0                                      1.000000                           1.0   \n",
       "0                                      0.000000                           0.0   \n",
       "1                                      0.500000                           0.0   \n",
       "\n",
       "                 ...  Length_of_URL  Inlink_number  Outlink_number  PageRank  \\\n",
       "relevance_label  ...                                                           \n",
       "2                ...             62       11089534               2       116   \n",
       "2                ...             54       11089534               2       124   \n",
       "0                ...             45              3               1       124   \n",
       "2                ...             56       11089534              13       123   \n",
       "1                ...             64              5               7       256   \n",
       "...              ...            ...            ...             ...       ...   \n",
       "0                ...             39              7               2     13829   \n",
       "0                ...             28              0               0     26074   \n",
       "0                ...             58             11               2      2995   \n",
       "0                ...             65              0               0       138   \n",
       "1                ...             30            131               0     13556   \n",
       "\n",
       "                 SiteRank  QualityScore  QualityScore2  Query-url_click_count  \\\n",
       "relevance_label                                                                 \n",
       "2                   64034            13              3                      0   \n",
       "2                   64034             1              2                      0   \n",
       "0                    3344            14             67                      0   \n",
       "2                   63933             1              3                      0   \n",
       "1                   49697             1             13                      0   \n",
       "...                   ...           ...            ...                    ...   \n",
       "0                   35302            21              8                      0   \n",
       "0                   35101            14              7                      1   \n",
       "0                   62170             4              8                      0   \n",
       "0                   56419             4              3                      0   \n",
       "1                   25675             2             12                      0   \n",
       "\n",
       "                 url_click_count  url_dwell_time  \n",
       "relevance_label                                   \n",
       "2                              0             0.0  \n",
       "2                              0             0.0  \n",
       "0                              0             0.0  \n",
       "2                              0             0.0  \n",
       "1                              0             0.0  \n",
       "...                          ...             ...  \n",
       "0                              0             0.0  \n",
       "0                              0             0.0  \n",
       "0                              0             0.0  \n",
       "0                              0             0.0  \n",
       "1                              0             0.0  \n",
       "\n",
       "[723412 rows x 137 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "drop_unwanted_cols(5)\n",
    "tmp = pd.read_csv(f'MLSR/Fold1/df_train.csv', index_col=0)\n",
    "display(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert CSV to TFRecord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SVM_to_TFRecord():\n",
    "    '''SVM_to_TFRecord converts .txt files to TFRecords using sklearns load_svmlight_file which \n",
    "       loads the files in SVMlight format allowing faster computations. Loading .txt with this\n",
    "       method returns three arrays when query_id is set to True. \n",
    "       \n",
    "           Array 0 = Features\n",
    "           Array 1 = Relevance\n",
    "           Array 2 = Query ID\n",
    "           \n",
    "      SVM_to_TFRecord Args:\n",
    "                          data_path: Path to load .txt data\n",
    "                          tfrecord_path: Path to save .tfrecords           \n",
    "           '''\n",
    "    \n",
    "    def __init__(self, data_path:str='', tfrecord_path:str=''):\n",
    "        self.data_path = data_path\n",
    "        self.tfrecord_path = tfrecord_path\n",
    "        \n",
    "        if not os.path.isdir(self.data_path):\n",
    "            print('Please enter correct data path!!!!')\n",
    "\n",
    "        if not os.path.isdir(self.tfrecord_path):\n",
    "            os.makedirs(self.tfrecord_path)\n",
    "\n",
    "    # Helper functions for creating TF Features\n",
    "    # Note: There are only two feature types in \n",
    "    # the datasets\n",
    "    def _float_feature(self, value):\n",
    "      \"\"\"Returns a float_list from a float / double.\"\"\"\n",
    "      return tf.train.Feature(float_list=tf.train.FloatList(value=[value]))\n",
    "\n",
    "    def _int64_feature(self, value):\n",
    "      \"\"\"Returns an int64_list from a bool / enum / int / uint.\"\"\"\n",
    "      return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))\n",
    "    \n",
    "    def create_tfrecord(self, data, tfrecord_path):\n",
    "        \n",
    "        options_ = tf.io.TFRecordOptions(compression_type='GZIP')\n",
    "        with tf.io.TFRecordWriter(tfrecord_path) as writer:\n",
    "\n",
    "            # Create Example list \n",
    "            elwc = input_pb2.ExampleListWithContext()\n",
    "            # Save the last query id for filtering\n",
    "            last_query_id = None\n",
    "            # Map Feature names to dictionary\n",
    "            features_ = preprocess_features()\n",
    "            # Extract column names\n",
    "            labels = np.array(features_['cols'])\n",
    "\n",
    "            for row in range(data.shape[0]):\n",
    "                # Select data from each row\n",
    "                relevance_label, query_id, features = data[row,0], data[row,1], data[row,2:]\n",
    "                # Create Example Dict\n",
    "                example_dict = {\n",
    "                    f'{feat_name}':self._float_feature(feat_val)\n",
    "                    for feat_name, feat_val in zip(labels, features)\n",
    "                }\n",
    "                example_dict['relevance_label'] = self._int64_feature(int(relevance_label))\n",
    "                # Create Features\n",
    "                example_ = tf.train.Example(features=tf.train.Features(feature=example_dict))\n",
    "                # Create ELWC by query id\n",
    "                if query_id != last_query_id:\n",
    "                    if last_query_id != None:\n",
    "                        writer.write(elwc.SerializeToString())\n",
    "                    last_query_id = query_id\n",
    "                    elwc = input_pb2.ExampleListWithContext()\n",
    "                    elwc.examples.append(example_)\n",
    "                else:\n",
    "                    elwc.examples.append(example_)\n",
    "            # Writing the final query\n",
    "            writer.write(elwc.SerializeToString())\n",
    "            \n",
    "    def load_and_convert_data(self):\n",
    "        \n",
    "        train = load_svmlight_file(f'{self.data_path}/train.txt', query_id=True)\n",
    "        test = load_svmlight_file(f'{self.data_path}/test.txt', query_id=True)\n",
    "        val = load_svmlight_file(f'{self.data_path}/vali.txt', query_id=True)\n",
    "        \n",
    "        # Note: train, test, val each return three numpy arrays:\n",
    "            # Array 0 = features\n",
    "            # Array 1 = relevance\n",
    "            # Array 3 = Query IDences in a batch fit a given \n",
    "            \n",
    "        data_ = {'train':train, 'test':test, 'val':val}\n",
    "        \n",
    "        # Concatenate data into one array\n",
    "        for label, data in data_.items():\n",
    "            if label == 'train':\n",
    "                train = np.concatenate((np.expand_dims(train[1], axis=1), np.expand_dims(train[2], axis=1), train[0].toarray()), axis=1)\n",
    "                tfrecord_path = f'{self.tfrecord_path}/train.tfrecords'\n",
    "                self.create_tfrecord(train, tfrecord_path)\n",
    "                \n",
    "            elif label == 'test':\n",
    "                test = np.concatenate((np.expand_dims(test[1], axis=1), np.expand_dims(test[2], axis=1), test[0].toarray()), axis=1)\n",
    "                tfrecord_path = f'{self.tfrecord_path}/test.tfrecords'\n",
    "                self.create_tfrecord(test, tfrecord_path)\n",
    "\n",
    "            else:\n",
    "                val = np.concatenate((np.expand_dims(val[1], axis=1), np.expand_dims(val[2], axis=1), val[0].toarray()), axis=1)\n",
    "                tfrecord_path = f'{self.tfrecord_path}/val.tfrecords'\n",
    "                self.create_tfrecord(val, tfrecord_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UserWarning: Ray execution environment not yet initialized. Initializing...\n",
      "To remove this warning, run the following python code before doing dataframe operations:\n",
      "\n",
      "    import ray\n",
      "    ray.init()\n",
      "\n",
      "2024-06-03 09:55:43,527\tINFO worker.py:1752 -- Started a local Ray instance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed Fold1/tfrecord_1\n",
      "Completed Fold2/tfrecord_2\n",
      "Completed Fold3/tfrecord_3\n",
      "Completed Fold4/tfrecord_4\n",
      "Completed Fold5/tfrecord_5\n"
     ]
    }
   ],
   "source": [
    "data_path = ['Fold1', 'Fold2', 'Fold3', 'Fold4', 'Fold5']\n",
    "\n",
    "for data in data_path:\n",
    "    path = f\"tfrecord_{data.lstrip('Fold')}\"\n",
    "    converter = SVM_to_TFRecord(f\"MLSR/{data}\", path)\n",
    "    converter.load_and_convert_data()\n",
    "    print(f'Completed {data}/{path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(model_dir,\n",
    "                train_input_pattern:str='tfrecord_1/train.tfrecords',\n",
    "                valid_input_pattern:str='tfrecord_1/val.tfrecords',\n",
    "                num_epochs:int=5,\n",
    "                optimizer:str='adagrad',\n",
    "                loss:str=\"approx_ndcg_loss\",\n",
    "                hidden_layer_dims:list=[1024,512,256],\n",
    "                activation=tf.nn.relu,\n",
    "                use_batch_norm:bool=True,\n",
    "                batch_norm_moment:float=0.999,\n",
    "                dropout:float=0.5,\n",
    "                list_size:int=200,\n",
    "                preprocessor:bool=False,\n",
    "                train_batch_size:int=128,\n",
    "                valid_batch_size:int=128,\n",
    "                steps_per_epoch:int=5000,\n",
    "                validation_steps:int=125,\n",
    "                learning_rate:float=0.05):\n",
    "    \n",
    "    '''Trains and validates data via TensorFlowRanking Pipeline with DNNScorer\n",
    "    \n",
    "    Args:\n",
    "        model_dir: Directory location to save modeling information (Note: Code will generate new folder)\n",
    "        train_input_pattern: training data file location\n",
    "        valid_input_pattern: validation data file location\n",
    "        optimizer: tf.keras.optimizers\n",
    "        num_epochs: total training epochs\n",
    "        loss: model loss function --> default: 'approx_ndcg_loss'\n",
    "        hidden_layer_dims: hidden layer dimensions\n",
    "        activation: activation for each layer\n",
    "        use_batch_norm: use of batch normalization for each layer\n",
    "        batch_norm_moment: momentum for the moving average in batch normalization\n",
    "        dropout: dropout rate\n",
    "        preprocessor: use of preprosser (Note: Current implmentation = log1p)\n",
    "        train_batch_size: training batch size\n",
    "        valid_batch_size: validation batch size\n",
    "        steps_per_epoch: number of training steps per epoch\n",
    "        validation_steps: number of validation steps per epoch\n",
    "        learning_rate: learning rate     \n",
    "    '''\n",
    "    \n",
    "    # Define Scorer\n",
    "    scorer = tfr.keras.model.DNNScorer(hidden_layer_dims=hidden_layer_dims,\n",
    "                                        output_units=1,\n",
    "                                        activation=activation,\n",
    "                                        use_batch_norm=use_batch_norm,\n",
    "                                        batch_norm_moment=batch_norm_moment,\n",
    "                                        dropout=dropout)\n",
    "\n",
    "    # Collect feature names for mapping\n",
    "    features = preprocess_features()\n",
    "    feature_cols = np.array(features['cols'])\n",
    "    \n",
    "    # Create specs for pipeline\n",
    "    context_spec_ = {}\n",
    "    example_spec_ = {feat: tf.io.FixedLenFeature(shape=(1,), dtype=tf.float32, default_value=0.0) for feat in feature_cols}\n",
    "    label_spec_ = ('relevance_label', tf.io.FixedLenFeature(shape=(1,), dtype=tf.int64, default_value=-1))\n",
    "\n",
    "    # Log1p preprocessing spec for data normalization\n",
    "    preprocess_spec = {\n",
    "        **{name: lambda t: tf.math.log1p(t * tf.sign(t)) * tf.sign(t)\n",
    "           for name in example_spec_.keys()}\n",
    "    }\n",
    "\n",
    "    # Define Input Creator\n",
    "    input_creator= tfr.keras.model.FeatureSpecInputCreator(\n",
    "            context_feature_spec={},\n",
    "            example_feature_spec=example_spec_)\n",
    "    \n",
    "    # Define Model\n",
    "    if preprocessor == False:\n",
    "        model_builder = tfr.keras.model.ModelBuilder(\n",
    "            input_creator=input_creator,\n",
    "            preprocessor=tfr.keras.model.PreprocessorWithSpec(),\n",
    "            scorer=scorer,\n",
    "            mask_feature_name=\"example_list_mask\",\n",
    "            name=\"model_builder\")\n",
    "    else:\n",
    "        model_builder = tfr.keras.model.ModelBuilder(\n",
    "        input_creator=input_creator,\n",
    "        preprocessor=tfr.keras.model.PreprocessorWithSpec(preprocess_spec),\n",
    "        scorer=scorer,\n",
    "        mask_feature_name=\"example_list_mask\",\n",
    "        name=\"model_builder\")\n",
    "\n",
    "    # Define Dataset Parameters\n",
    "    dataset_hparams = tfr.keras.pipeline.DatasetHparams(\n",
    "        train_input_pattern=train_input_pattern,\n",
    "        valid_input_pattern=valid_input_pattern,\n",
    "        train_batch_size=train_batch_size,\n",
    "        valid_batch_size=valid_batch_size,\n",
    "        list_size=list_size,\n",
    "        dataset_reader=tfr.keras.pipeline.DatasetHparams.dataset_reader)\n",
    "\n",
    "    # Define Dataset Builder\n",
    "    dataset_builder = tfr.keras.pipeline.SimpleDatasetBuilder(\n",
    "        {},\n",
    "        example_spec_,\n",
    "        mask_feature_name=\"example_list_mask\",\n",
    "        label_spec=label_spec_,\n",
    "        hparams=dataset_hparams,\n",
    "        sample_weight_spec=None)\n",
    "\n",
    "    # Define Pipeline Hparams\n",
    "    pipeline_hparams = tfr.keras.pipeline.PipelineHparams(\n",
    "        model_dir=model_dir,\n",
    "        num_epochs=num_epochs,\n",
    "        steps_per_epoch=steps_per_epoch,\n",
    "        validation_steps=validation_steps,\n",
    "        learning_rate=learning_rate,\n",
    "        loss=loss,\n",
    "        optimizer=optimizer)\n",
    "    \n",
    "    # Define Ranking Pipeline\n",
    "    ranking_pipeline = tfr.keras.pipeline.SimplePipeline(\n",
    "        model_builder,\n",
    "        dataset_builder=dataset_builder,\n",
    "        hparams=pipeline_hparams)\n",
    "    \n",
    "    # Train and Validate\n",
    "    ranking_pipeline.train_and_validate(verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UserWarning: Ray execution environment not yet initialized. Initializing...\n",
      "To remove this warning, run the following python code before doing dataframe operations:\n",
      "\n",
      "    import ray\n",
      "    ray.init()\n",
      "\n",
      "2024-06-03 11:26:57,450\tINFO worker.py:1752 -- Started a local Ray instance.\n",
      "2024-06-03 11:27:12.395714: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1960] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`tf.keras.callbacks.experimental.BackupAndRestore` endpoint is deprecated and will be removed in a future release. Please use `tf.keras.callbacks.BackupAndRestore`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`tf.keras.callbacks.experimental.BackupAndRestore` endpoint is deprecated and will be removed in a future release. Please use `tf.keras.callbacks.BackupAndRestore`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "5000/5000 [==============================] - 3923s 785ms/step - loss: -0.6773 - metric/ndcg_1: 0.4123 - metric/ndcg_5: 0.4071 - metric/ndcg_10: 0.4255 - metric/ndcg: 0.6785 - val_loss: -0.6916 - val_metric/ndcg_1: 0.4433 - val_metric/ndcg_5: 0.4338 - val_metric/ndcg_10: 0.4523 - val_metric/ndcg: 0.6930\n",
      "Epoch 2/5\n",
      "5000/5000 [==============================] - 3838s 768ms/step - loss: -0.6884 - metric/ndcg_1: 0.4366 - metric/ndcg_5: 0.4256 - metric/ndcg_10: 0.4434 - metric/ndcg: 0.6891 - val_loss: -0.6944 - val_metric/ndcg_1: 0.4442 - val_metric/ndcg_5: 0.4359 - val_metric/ndcg_10: 0.4567 - val_metric/ndcg: 0.6953\n",
      "Epoch 3/5\n",
      "5000/5000 [==============================] - 3832s 766ms/step - loss: -0.6914 - metric/ndcg_1: 0.4431 - metric/ndcg_5: 0.4309 - metric/ndcg_10: 0.4483 - metric/ndcg: 0.6921 - val_loss: -0.6950 - val_metric/ndcg_1: 0.4416 - val_metric/ndcg_5: 0.4380 - val_metric/ndcg_10: 0.4574 - val_metric/ndcg: 0.6957\n",
      "Epoch 4/5\n",
      "5000/5000 [==============================] - 3713s 743ms/step - loss: -0.6934 - metric/ndcg_1: 0.4487 - metric/ndcg_5: 0.4343 - metric/ndcg_10: 0.4515 - metric/ndcg: 0.6941 - val_loss: -0.6959 - val_metric/ndcg_1: 0.4454 - val_metric/ndcg_5: 0.4405 - val_metric/ndcg_10: 0.4594 - val_metric/ndcg: 0.6968\n",
      "Epoch 5/5\n",
      "5000/5000 [==============================] - 3785s 757ms/step - loss: -0.6951 - metric/ndcg_1: 0.4526 - metric/ndcg_5: 0.4376 - metric/ndcg_10: 0.4547 - metric/ndcg: 0.6957 - val_loss: -0.6951 - val_metric/ndcg_1: 0.4418 - val_metric/ndcg_5: 0.4370 - val_metric/ndcg_10: 0.4581 - val_metric/ndcg: 0.6960\n",
      "INFO:tensorflow:Assets written to: base_model_with_preprocessing/export/latest_model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: base_model_with_preprocessing/export/latest_model/assets\n"
     ]
    }
   ],
   "source": [
    "build_model(preprocessor=True, model_dir='base_model_with_preprocessing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UserWarning: Ray execution environment not yet initialized. Initializing...\n",
      "To remove this warning, run the following python code before doing dataframe operations:\n",
      "\n",
      "    import ray\n",
      "    ray.init()\n",
      "\n",
      "2024-06-04 10:23:24,173\tINFO worker.py:1752 -- Started a local Ray instance.\n",
      "2024-06-04 10:23:38.478960: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1960] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000/2000 [==============================] - 449s 221ms/step - loss: -0.6915 - metric/ndcg_1: 0.4384 - metric/ndcg_5: 0.4331 - metric/ndcg_10: 0.4517 - metric/ndcg: 0.6923\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UserWarning: Distributing <class 'list'> object. This may take some time.\n"
     ]
    }
   ],
   "source": [
    "# Build Test Dataset for Model Ingestion\n",
    "\n",
    "features = preprocess_features()\n",
    "feature_cols = np.array(features['cols'])\n",
    "\n",
    "# Create specs for pipeline\n",
    "context_spec_ = {}\n",
    "example_spec_ = {feat: tf.io.FixedLenFeature(shape=(1,), dtype=tf.float32, default_value=0.0) for feat in feature_cols}\n",
    "label_spec_ = ('relevance_label', tf.io.FixedLenFeature(shape=(1,), dtype=tf.int64, default_value=-1))\n",
    "\n",
    "dataset_hparams = tfr.keras.pipeline.DatasetHparams(\n",
    "    train_input_pattern='tfrecord_1/train.tfrecords',\n",
    "    valid_input_pattern='tfrecord_1/test.tfrecords',\n",
    "    train_batch_size=128,\n",
    "    valid_batch_size=128,\n",
    "    list_size=200,\n",
    "    dataset_reader=tfr.keras.pipeline.DatasetHparams.dataset_reader)\n",
    "\n",
    "# Define Dataset Builder\n",
    "dataset_builder = tfr.keras.pipeline.SimpleDatasetBuilder(\n",
    "    {},\n",
    "    example_spec_,\n",
    "    mask_feature_name=\"example_list_mask\",\n",
    "    label_spec=label_spec_,\n",
    "    hparams=dataset_hparams,\n",
    "    sample_weight_spec=None)\n",
    "\n",
    "ds = dataset_builder.build_valid_dataset()\n",
    "\n",
    "model_with_preprocessing = tf.keras.models.load_model('base_model_with_preprocessing/export/latest_model/')\n",
    "\n",
    "loss_with_preprocessing = model_with_preprocessing.evaluate(ds.take(2000))\n",
    "\n",
    "with_preprocessing = pd.DataFrame(loss_with_preprocessing, columns=['With_Preprocessing'], index=['Loss', 'NDCG_1', 'NDCG_5', 'NDCG_10', 'NDCG']).T\n",
    "\n",
    "final_results = with_preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Loss</th>\n",
       "      <th>NDCG_1</th>\n",
       "      <th>NDCG_5</th>\n",
       "      <th>NDCG_10</th>\n",
       "      <th>NDCG</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>With_Preprocessing</th>\n",
       "      <td>-0.691515</td>\n",
       "      <td>0.438447</td>\n",
       "      <td>0.433117</td>\n",
       "      <td>0.451722</td>\n",
       "      <td>0.692254</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        Loss    NDCG_1    NDCG_5   NDCG_10      NDCG\n",
       "With_Preprocessing -0.691515  0.438447  0.433117  0.451722  0.692254"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ERROR: Failed to launch TensorBoard (exited with 1).\n",
       "Contents of stderr:\n",
       "2024-06-03 18:05:54.156108: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
       "2024-06-03 18:05:54.244780: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
       "2024-06-03 18:05:54.246942: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
       "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
       "2024-06-03 18:05:55.489124: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
       "2024-06-03 18:05:56.843829: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1960] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
       "Skipping registering GPU devices...\n",
       "E0603 18:05:56.866194 140387424698624 application.py:125] Failed to load plugin WhatIfToolPluginLoader.load; ignoring it.\n",
       "Traceback (most recent call last):\n",
       "  File \"/home/amytrai/anaconda3/envs/ltr/lib/python3.8/site-packages/tensorboard/backend/application.py\", line 123, in TensorBoardWSGIApp\n",
       "    plugin = loader.load(context)\n",
       "  File \"/home/amytrai/anaconda3/envs/ltr/lib/python3.8/site-packages/tensorboard_plugin_wit/wit_plugin_loader.py\", line 57, in load\n",
       "    from tensorboard_plugin_wit.wit_plugin import WhatIfToolPlugin\n",
       "  File \"/home/amytrai/anaconda3/envs/ltr/lib/python3.8/site-packages/tensorboard_plugin_wit/wit_plugin.py\", line 40, in <module>\n",
       "    from tensorboard_plugin_wit._utils import common_utils\n",
       "  File \"/home/amytrai/anaconda3/envs/ltr/lib/python3.8/site-packages/tensorboard_plugin_wit/_utils/common_utils.py\", line 17, in <module>\n",
       "    from tensorboard_plugin_wit._vendor.tensorflow_serving.apis import classification_pb2\n",
       "  File \"/home/amytrai/anaconda3/envs/ltr/lib/python3.8/site-packages/tensorboard_plugin_wit/_vendor/tensorflow_serving/apis/classification_pb2.py\", line 15, in <module>\n",
       "    from tensorboard_plugin_wit._vendor.tensorflow_serving.apis import input_pb2 as tensorflow__serving_dot_apis_dot_input__pb2\n",
       "  File \"/home/amytrai/anaconda3/envs/ltr/lib/python3.8/site-packages/tensorboard_plugin_wit/_vendor/tensorflow_serving/apis/input_pb2.py\", line 37, in <module>\n",
       "    _descriptor.FieldDescriptor(\n",
       "  File \"/home/amytrai/.local/lib/python3.8/site-packages/google/protobuf/descriptor.py\", line 621, in __new__\n",
       "    _message.Message._CheckCalledFromGeneratedFile()\n",
       "TypeError: Descriptors cannot be created directly.\n",
       "If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.\n",
       "If you cannot immediately regenerate your protos, some other possible workarounds are:\n",
       " 1. Downgrade the protobuf package to 3.20.x or lower.\n",
       " 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).\n",
       "\n",
       "More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates\n",
       "Address already in use\n",
       "Port 6006 is in use by another program. Either identify and stop that program, or start the server with a different port."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Evaluate model performance\n",
    "%reload_ext tensorboard\n",
    "\n",
    "%tensorboard --logdir base_model_with_preprocessing/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x, y in ds.take(1):\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "137\n"
     ]
    }
   ],
   "source": [
    "print(len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 3s 61ms/step\n",
      "score: [[ -1.5973206  -15.563038    -7.1940303  ... -23.02585    -23.02585\n",
      "  -23.02585   ]\n",
      " [-16.63225     -0.13893722   1.5003042  ... -23.02585    -23.02585\n",
      "  -23.02585   ]\n",
      " [ -5.431166     3.6945255    0.2204494  ... -23.02585    -23.02585\n",
      "  -23.02585   ]\n",
      " ...\n",
      " [  1.852209     2.557881     2.618662   ... -23.02585    -23.02585\n",
      "  -23.02585   ]\n",
      " [ -4.5550756   -9.186165   -19.309807   ... -23.02585    -23.02585\n",
      "  -23.02585   ]\n",
      " [  1.418303    -2.9753964    9.088194   ... -23.02585    -23.02585\n",
      "  -23.02585   ]]\n"
     ]
    }
   ],
   "source": [
    "score = model_with_preprocessing.predict(x)\n",
    "print(f\"score: {score}\")\n",
    "min_score = tf.reduce_min(score)\n",
    "scores = tf.where(tf.greater_equal(y, 0.), score, min_score - 1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(128, 200)\n"
     ]
    }
   ],
   "source": [
    "print(score.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[ -1.5973206  -15.563038    -7.1940303  ... -67.37445    -67.37445\n",
      "  -67.37445   ]\n",
      " [-16.63225     -0.13893722   1.5003042  ... -67.37445    -67.37445\n",
      "  -67.37445   ]\n",
      " [ -5.431166     3.6945255    0.2204494  ... -67.37445    -67.37445\n",
      "  -67.37445   ]\n",
      " ...\n",
      " [  1.852209     2.557881     2.618662   ... -67.37445    -67.37445\n",
      "  -67.37445   ]\n",
      " [ -4.5550756   -9.186165   -19.309807   ... -67.37445    -67.37445\n",
      "  -67.37445   ]\n",
      " [  1.418303    -2.9753964    9.088194   ... -67.37445    -67.37445\n",
      "  -67.37445   ]], shape=(128, 200), dtype=float32)\n",
      "(128, 200)\n"
     ]
    }
   ],
   "source": [
    "print(scores)\n",
    "print(scores.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_idx = 0\n",
    "batch_scores = scores[batch_idx]\n",
    "batch_feature = x['QualityScore'][batch_idx]\n",
    "batch_scores = scores[batch_idx:batch_idx+1]  # Shape: [1, list_size]\n",
    "batch_feature = x['QualityScore'][batch_idx:batch_idx+1]  # Shape: [1, list_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[ -1.5973206  -15.563038    -7.1940303   15.137175   -21.49283\n",
      "   17.021065    -9.184447    -0.39191926  20.675968     1.6588693\n",
      "   20.131027   -12.241798    15.923479    39.361       17.02776\n",
      "   13.912825    19.912148    24.841759    16.006718    -9.9095545\n",
      "  -30.8385      11.960856    12.485785     0.49007228   2.3352404\n",
      "    1.0140514  -16.398066     7.9764      20.135534    11.439901\n",
      "   16.796497     3.9551904   -2.1022117    2.0299447   42.725243\n",
      "    6.2371097   -0.7471613   27.294209    25.002216   -15.886215\n",
      "    1.1301229    3.0540493  -10.876614   -14.174013    11.431093\n",
      "  -11.374411   -17.353363    49.800896    24.909369    -9.049595\n",
      "    8.359512     8.777021     2.380428     2.5781476   24.576576\n",
      "   61.554        1.967894    32.933594    27.445902   -33.458908\n",
      "   27.565468    12.552557    19.824291    44.633293    16.072903\n",
      "    1.2291312   12.370058    11.935087     2.7311378    8.730278\n",
      "   -1.5253975   18.925201    -8.471321    20.274052     1.8754278\n",
      "    7.230947    41.46808     -2.8302627    5.016034   -21.931963\n",
      "   19.797752   -24.085468     2.7752144    0.86748487  11.316817\n",
      "   37.965378    40.94027     11.12063     39.270935    43.5361\n",
      "    5.511232    17.587683    21.127789    -3.196263    13.317891\n",
      "   27.195404    20.274412    54.422794    17.269363    15.934684\n",
      "    3.3140583   11.200719    15.374859    -5.873894    26.779324\n",
      "   -3.3387535    0.13056745 -30.309807    -8.565564     1.4285996\n",
      "  -31.915936    45.63998      3.36458     34.038315    47.309658\n",
      "    7.924576     2.1805475   15.216166    14.919438    -5.002151\n",
      "   19.899616   -41.357574     0.9413185   42.354652     7.4031467\n",
      "    3.2063518   20.293596    42.29808      5.037474   -19.279175\n",
      "   10.066423    39.158524   -14.533509    43.205677     0.23042068\n",
      "   30.2118     -15.6322      30.375221   -67.37445    -67.37445\n",
      "  -67.37445    -67.37445    -67.37445    -67.37445    -67.37445\n",
      "  -67.37445    -67.37445    -67.37445    -67.37445    -67.37445\n",
      "  -67.37445    -67.37445    -67.37445    -67.37445    -67.37445\n",
      "  -67.37445    -67.37445    -67.37445    -67.37445    -67.37445\n",
      "  -67.37445    -67.37445    -67.37445    -67.37445    -67.37445\n",
      "  -67.37445    -67.37445    -67.37445    -67.37445    -67.37445\n",
      "  -67.37445    -67.37445    -67.37445    -67.37445    -67.37445\n",
      "  -67.37445    -67.37445    -67.37445    -67.37445    -67.37445\n",
      "  -67.37445    -67.37445    -67.37445    -67.37445    -67.37445\n",
      "  -67.37445    -67.37445    -67.37445    -67.37445    -67.37445\n",
      "  -67.37445    -67.37445    -67.37445    -67.37445    -67.37445\n",
      "  -67.37445    -67.37445    -67.37445    -67.37445    -67.37445   ]], shape=(1, 200), dtype=float32)\n",
      "(1, 200)\n"
     ]
    }
   ],
   "source": [
    "print(batch_scores)\n",
    "print(batch_scores.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1,), dtype=int64, numpy=array([55])>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.argmax(batch_scores, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[[ 28.]\n",
      "  [ 65.]\n",
      "  [112.]\n",
      "  [  1.]\n",
      "  [ 15.]\n",
      "  [  1.]\n",
      "  [  3.]\n",
      "  [  1.]\n",
      "  [ 16.]\n",
      "  [129.]\n",
      "  [ 15.]\n",
      "  [  5.]\n",
      "  [  7.]\n",
      "  [ 16.]\n",
      "  [  5.]\n",
      "  [ 48.]\n",
      "  [  8.]\n",
      "  [ 13.]\n",
      "  [  2.]\n",
      "  [  9.]\n",
      "  [  6.]\n",
      "  [ 12.]\n",
      "  [  6.]\n",
      "  [  1.]\n",
      "  [  3.]\n",
      "  [132.]\n",
      "  [  2.]\n",
      "  [  7.]\n",
      "  [ 25.]\n",
      "  [ 21.]\n",
      "  [185.]\n",
      "  [  6.]\n",
      "  [  1.]\n",
      "  [  9.]\n",
      "  [ 18.]\n",
      "  [116.]\n",
      "  [  4.]\n",
      "  [  2.]\n",
      "  [ 40.]\n",
      "  [ 69.]\n",
      "  [ 79.]\n",
      "  [  7.]\n",
      "  [ 14.]\n",
      "  [ 27.]\n",
      "  [  4.]\n",
      "  [ 52.]\n",
      "  [ 28.]\n",
      "  [ 16.]\n",
      "  [ 97.]\n",
      "  [ 15.]\n",
      "  [  6.]\n",
      "  [  9.]\n",
      "  [ 20.]\n",
      "  [ 58.]\n",
      "  [  1.]\n",
      "  [ 27.]\n",
      "  [ 77.]\n",
      "  [ 21.]\n",
      "  [ 15.]\n",
      "  [ 51.]\n",
      "  [  7.]\n",
      "  [  1.]\n",
      "  [ 32.]\n",
      "  [  1.]\n",
      "  [ 16.]\n",
      "  [ 15.]\n",
      "  [ 31.]\n",
      "  [ 15.]\n",
      "  [  1.]\n",
      "  [156.]\n",
      "  [ 14.]\n",
      "  [  4.]\n",
      "  [  6.]\n",
      "  [  2.]\n",
      "  [ 33.]\n",
      "  [ 31.]\n",
      "  [ 33.]\n",
      "  [  5.]\n",
      "  [ 19.]\n",
      "  [  1.]\n",
      "  [ 39.]\n",
      "  [  1.]\n",
      "  [  9.]\n",
      "  [ 34.]\n",
      "  [  1.]\n",
      "  [ 29.]\n",
      "  [ 20.]\n",
      "  [248.]\n",
      "  [ 31.]\n",
      "  [  6.]\n",
      "  [  1.]\n",
      "  [  1.]\n",
      "  [ 12.]\n",
      "  [  1.]\n",
      "  [ 14.]\n",
      "  [  4.]\n",
      "  [  9.]\n",
      "  [ 34.]\n",
      "  [  2.]\n",
      "  [ 10.]\n",
      "  [  5.]\n",
      "  [  7.]\n",
      "  [ 10.]\n",
      "  [  1.]\n",
      "  [ 21.]\n",
      "  [  8.]\n",
      "  [  4.]\n",
      "  [  7.]\n",
      "  [ 44.]\n",
      "  [ 22.]\n",
      "  [  3.]\n",
      "  [  2.]\n",
      "  [ 14.]\n",
      "  [  9.]\n",
      "  [  8.]\n",
      "  [  6.]\n",
      "  [  9.]\n",
      "  [  6.]\n",
      "  [  5.]\n",
      "  [ 31.]\n",
      "  [140.]\n",
      "  [  1.]\n",
      "  [ 29.]\n",
      "  [ 30.]\n",
      "  [ 65.]\n",
      "  [ 27.]\n",
      "  [ 12.]\n",
      "  [ 33.]\n",
      "  [ 13.]\n",
      "  [  3.]\n",
      "  [ 21.]\n",
      "  [ 14.]\n",
      "  [  3.]\n",
      "  [  2.]\n",
      "  [ 13.]\n",
      "  [  2.]\n",
      "  [  4.]\n",
      "  [  3.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]]], shape=(1, 200, 1), dtype=float32)\n",
      "(1, 200, 1)\n"
     ]
    }
   ],
   "source": [
    "print(batch_feature)\n",
    "print(batch_feature.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(27.0, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(batch_feature[0][55][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sorted answers: [ 27.  34.  16.   8.   2.   1.   6.   2.  18.  30.  33.  33.  20.  16.\n",
      "  31.  14.  29.   9.  21.   3.   2.   7.  15.   2.   4.  21.  40.  97.\n",
      "  13.   1.  12.  16.  12.   9.   2.  25.  15.   8. 140.  32.  39.   4.\n",
      "   1.   2.   5.   1. 185.  16.   2.  10.   7.  10.   6.   1.   5.  48.\n",
      "  14.   1.   6.  31.  12.  15.  21.   4.   1.   7. 248.  21.   9. 156.\n",
      "   6.   7.   6.  65.  31. 116.   1.  13.  19.   6.  14.   5.  27.   7.\n",
      "   9.   1.  58.  20.   3.   9.   9.  77.  33. 129.  22.  15.  79. 132.\n",
      "  29.  34.   1.  13.   4.   1.   4.  14.  28.   1.   5.   1.   8.  31.\n",
      "   1. 112.   6.  44.  15.   3.   9.  14.  52.   5.  27.   3.  65.   4.\n",
      "  69.   2.  28.   3.  15.   1.   1.   7.   6.   3.  51.   1.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.]\n",
      "(200,)\n"
     ]
    }
   ],
   "source": [
    "# Sort by scores\n",
    "sorted_indices = tfr.utils.sort_by_scores(\n",
    "    batch_scores,\n",
    "    [batch_feature]\n",
    ")[0]  # We only need the sorted indices\n",
    "\n",
    "# Collect sorted answers\n",
    "sorted_answers = []\n",
    "sorted_answers.append(sorted_indices)\n",
    "\n",
    "# Convert list of sorted answers to numpy array\n",
    "sorted_answers = np.array(sorted_answers)\n",
    "sorted_answers = np.reshape(sorted_answers, (-1,))\n",
    "print(f\"Sorted answers: {sorted_answers}\")\n",
    "\n",
    "print(sorted_answers.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['BM25_anchor', 'BM25_body', 'BM25_title', 'BM25_url', 'BM25_whole_document', 'IDF_Inverse_document_frequency__anchor', 'IDF_Inverse_document_frequency__body', 'IDF_Inverse_document_frequency__title', 'IDF_Inverse_document_frequency__url', 'IDF_Inverse_document_frequency__whole_document', 'Inlink_number', 'LMIR.ABS_anchor', 'LMIR.ABS_body', 'LMIR.ABS_title', 'LMIR.ABS_url', 'LMIR.ABS_whole_document', 'LMIR.DIR_anchor', 'LMIR.DIR_body', 'LMIR.DIR_title', 'LMIR.DIR_url', 'LMIR.DIR_whole_document', 'LMIR.JM_anchor', 'LMIR.JM_body', 'LMIR.JM_title', 'LMIR.JM_url', 'LMIR.JM_whole_document', 'Length_of_URL', 'Number_of_slash_in_URL', 'Outlink_number', 'PageRank', 'QualityScore', 'QualityScore2', 'Query-url_click_count', 'SiteRank', 'boolean_model_anchor', 'boolean_model_body', 'boolean_model_title', 'boolean_model_url', 'boolean_model_whole_document', 'covered_query_term_number_anchor', 'covered_query_term_number_body', 'covered_query_term_number_title', 'covered_query_term_number_url', 'covered_query_term_number_whole_document', 'covered_query_term_ratio_anchor', 'covered_query_term_ratio_body', 'covered_query_term_ratio_title', 'covered_query_term_ratio_url', 'covered_query_term_ratio_whole_document', 'max_of_stream_length_normalized_term_frequency_anchor', 'max_of_stream_length_normalized_term_frequency_body', 'max_of_stream_length_normalized_term_frequency_title', 'max_of_stream_length_normalized_term_frequency_url', 'max_of_stream_length_normalized_term_frequency_whole_document', 'max_of_term_frequency_anchor', 'max_of_term_frequency_body', 'max_of_term_frequency_title', 'max_of_term_frequency_url', 'max_of_term_frequency_whole_document', 'max_of_tf_idf_anchor', 'max_of_tf_idf_body', 'max_of_tf_idf_title', 'max_of_tf_idf_url', 'max_of_tf_idf_whole_document', 'mean_of_stream_length_normalized_term_frequency_anchor', 'mean_of_stream_length_normalized_term_frequency_body', 'mean_of_stream_length_normalized_term_frequency_title', 'mean_of_stream_length_normalized_term_frequency_url', 'mean_of_stream_length_normalized_term_frequency_whole_document', 'mean_of_term_frequency_anchor', 'mean_of_term_frequency_body', 'mean_of_term_frequency_title', 'mean_of_term_frequency_url', 'mean_of_term_frequency_whole_document', 'mean_of_tf_idf_anchor', 'mean_of_tf_idf_body', 'mean_of_tf_idf_title', 'mean_of_tf_idf_url', 'mean_of_tf_idf_whole_document', 'min_of_stream_length_normalized_term_frequency_anchor', 'min_of_stream_length_normalized_term_frequency_body', 'min_of_stream_length_normalized_term_frequency_title', 'min_of_stream_length_normalized_term_frequency_url', 'min_of_stream_length_normalized_term_frequency_whole_document', 'min_of_term_frequency_anchor', 'min_of_term_frequency_body', 'min_of_term_frequency_title', 'min_of_term_frequency_url', 'min_of_term_frequency_whole_document', 'min_of_tf_idf_anchor', 'min_of_tf_idf_body', 'min_of_tf_idf_title', 'min_of_tf_idf_url', 'min_of_tf_idf_whole_document', 'stream_length_anchor', 'stream_length_body', 'stream_length_title', 'stream_length_url', 'stream_length_whole_document', 'sum_of_stream_length_normalized_term_frequency_anchor', 'sum_of_stream_length_normalized_term_frequency_body', 'sum_of_stream_length_normalized_term_frequency_title', 'sum_of_stream_length_normalized_term_frequency_url', 'sum_of_stream_length_normalized_term_frequency_whole_document', 'sum_of_term_frequency_anchor', 'sum_of_term_frequency_body', 'sum_of_term_frequency_title', 'sum_of_term_frequency_url', 'sum_of_term_frequency_whole_document', 'sum_of_tf_idf_anchor', 'sum_of_tf_idf_body', 'sum_of_tf_idf_title', 'sum_of_tf_idf_url', 'sum_of_tf_idf_whole_document', 'url_click_count', 'url_dwell_time', 'variance_of_stream_length_normalized_term_frequency_anchor', 'variance_of_stream_length_normalized_term_frequency_body', 'variance_of_stream_length_normalized_term_frequency_title', 'variance_of_stream_length_normalized_term_frequency_url', 'variance_of_stream_length_normalized_term_frequency_whole_document', 'variance_of_term_frequency_anchor', 'variance_of_term_frequency_body', 'variance_of_term_frequency_title', 'variance_of_term_frequency_url', 'variance_of_term_frequency_whole_document', 'variance_of_tf_idf_anchor', 'variance_of_tf_idf_body', 'variance_of_tf_idf_title', 'variance_of_tf_idf_url', 'variance_of_tf_idf_whole_document', 'vector_space_model_anchor', 'vector_space_model_body', 'vector_space_model_title', 'vector_space_model_url', 'vector_space_model_whole_document', 'example_list_mask']\n",
      "137\n",
      "tf.Tensor(\n",
      "[[ 2.  1.  3. ... -1. -1. -1.]\n",
      " [ 0.  3.  3. ... -1. -1. -1.]\n",
      " [ 0.  0.  2. ... -1. -1. -1.]\n",
      " ...\n",
      " [ 1.  2.  1. ... -1. -1. -1.]\n",
      " [ 0.  0.  0. ... -1. -1. -1.]\n",
      " [ 0.  0.  1. ... -1. -1. -1.]], shape=(128, 200), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "keysList = list(x.keys())\n",
    "print(keysList)\n",
    "print(len(keysList))\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch_idx in range(scores.shape[0]):  # Assuming scores has shape (num_batches, list_size)\n",
    "    batch_scores = scores[batch_idx]\n",
    "    batch_feature = x['covered_query_term_number_body'][batch_idx]\n",
    "    \n",
    "    # Sort by scores\n",
    "    sorted_indices = tfr.utils.sort_by_scores(\n",
    "        batch_scores,\n",
    "        batch_feature\n",
    "    )[0]  # We only need the sorted indices\n",
    "\n",
    "    # Collect sorted answers\n",
    "    sorted_answers.append(sorted_indices)\n",
    "\n",
    "# Convert list of sorted answers to numpy array\n",
    "sorted_answers = np.array(sorted_answers)\n",
    "\n",
    "print(f\"Sorted answers: {sorted_answers}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000/2000 [==============================] - 394s 195ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UserWarning: Distributing <class 'numpy.ndarray'> object. This may take some time.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Shape of passed values is (256000, 200), indices imply (256000, 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m predictions \u001b[38;5;241m=\u001b[39m model_with_preprocessing\u001b[38;5;241m.\u001b[39mpredict(ds\u001b[38;5;241m.\u001b[39mtake(\u001b[38;5;241m2000\u001b[39m))\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Convert predictions to a pandas DataFrame if needed\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m predictions_df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpredictions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mScore\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Display the predictions\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(predictions_df)\n",
      "File \u001b[0;32m~/anaconda3/envs/ltr/lib/python3.8/site-packages/modin/logging/logger_decorator.py:128\u001b[0m, in \u001b[0;36menable_logging.<locals>.decorator.<locals>.run_and_log\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;124;03mCompute function with logging if Modin logging is enabled.\u001b[39;00m\n\u001b[1;32m    115\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[38;5;124;03mAny\u001b[39;00m\n\u001b[1;32m    126\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m LogMode\u001b[38;5;241m.\u001b[39mget() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdisable\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 128\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mobj\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    130\u001b[0m logger \u001b[38;5;241m=\u001b[39m get_logger()\n\u001b[1;32m    131\u001b[0m logger_level \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(logger, log_level)\n",
      "File \u001b[0;32m~/anaconda3/envs/ltr/lib/python3.8/site-packages/modin/pandas/dataframe.py:235\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[0;34m(self, data, index, columns, dtype, copy, query_compiler)\u001b[0m\n\u001b[1;32m    229\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m    231\u001b[0m         data \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    232\u001b[0m             k: v\u001b[38;5;241m.\u001b[39m_to_pandas() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(v, Series) \u001b[38;5;28;01melse\u001b[39;00m v\n\u001b[1;32m    233\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m data\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m    234\u001b[0m         }\n\u001b[0;32m--> 235\u001b[0m     pandas_df \u001b[38;5;241m=\u001b[39m \u001b[43mpandas\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDataFrame\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    236\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\n\u001b[1;32m    237\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    238\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_query_compiler \u001b[38;5;241m=\u001b[39m from_pandas(pandas_df)\u001b[38;5;241m.\u001b[39m_query_compiler\n\u001b[1;32m    239\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/ltr/lib/python3.8/site-packages/pandas/core/frame.py:758\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    747\u001b[0m         mgr \u001b[38;5;241m=\u001b[39m dict_to_mgr(\n\u001b[1;32m    748\u001b[0m             \u001b[38;5;66;03m# error: Item \"ndarray\" of \"Union[ndarray, Series, Index]\" has no\u001b[39;00m\n\u001b[1;32m    749\u001b[0m             \u001b[38;5;66;03m# attribute \"name\"\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    755\u001b[0m             copy\u001b[38;5;241m=\u001b[39m_copy,\n\u001b[1;32m    756\u001b[0m         )\n\u001b[1;32m    757\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 758\u001b[0m         mgr \u001b[38;5;241m=\u001b[39m \u001b[43mndarray_to_mgr\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    759\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    760\u001b[0m \u001b[43m            \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    761\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    762\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    763\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    764\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtyp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmanager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    765\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    767\u001b[0m \u001b[38;5;66;03m# For data is list-like, or Iterable (will consume into list)\u001b[39;00m\n\u001b[1;32m    768\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m is_list_like(data):\n",
      "File \u001b[0;32m~/anaconda3/envs/ltr/lib/python3.8/site-packages/pandas/core/internals/construction.py:337\u001b[0m, in \u001b[0;36mndarray_to_mgr\u001b[0;34m(values, index, columns, dtype, copy, typ)\u001b[0m\n\u001b[1;32m    332\u001b[0m \u001b[38;5;66;03m# _prep_ndarraylike ensures that values.ndim == 2 at this point\u001b[39;00m\n\u001b[1;32m    333\u001b[0m index, columns \u001b[38;5;241m=\u001b[39m _get_axes(\n\u001b[1;32m    334\u001b[0m     values\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], values\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], index\u001b[38;5;241m=\u001b[39mindex, columns\u001b[38;5;241m=\u001b[39mcolumns\n\u001b[1;32m    335\u001b[0m )\n\u001b[0;32m--> 337\u001b[0m \u001b[43m_check_values_indices_shape_match\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    339\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m typ \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marray\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    340\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28missubclass\u001b[39m(values\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mtype, \u001b[38;5;28mstr\u001b[39m):\n",
      "File \u001b[0;32m~/anaconda3/envs/ltr/lib/python3.8/site-packages/pandas/core/internals/construction.py:408\u001b[0m, in \u001b[0;36m_check_values_indices_shape_match\u001b[0;34m(values, index, columns)\u001b[0m\n\u001b[1;32m    406\u001b[0m passed \u001b[38;5;241m=\u001b[39m values\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m    407\u001b[0m implied \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mlen\u001b[39m(index), \u001b[38;5;28mlen\u001b[39m(columns))\n\u001b[0;32m--> 408\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShape of passed values is \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpassed\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, indices imply \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mimplied\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: Shape of passed values is (256000, 200), indices imply (256000, 1)"
     ]
    }
   ],
   "source": [
    "# Predict scores on the testing data\n",
    "predictions = model_with_preprocessing.predict(ds.take(2000))\n",
    "\n",
    "# Convert predictions to a pandas DataFrame if needed\n",
    "predictions_df = pd.DataFrame(predictions, columns=['Score'])\n",
    "\n",
    "# Display the predictions\n",
    "print(predictions_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ -1.5973206  -15.563038    -7.1940303  ... -23.02585    -23.02585\n",
      "  -23.02585   ]\n",
      " [-16.63225     -0.13893722   1.5003042  ... -23.02585    -23.02585\n",
      "  -23.02585   ]\n",
      " [ -5.431166     3.6945255    0.2204494  ... -23.02585    -23.02585\n",
      "  -23.02585   ]\n",
      " ...\n",
      " [-10.578133   -12.728604   -13.967123   ... -23.02585    -23.02585\n",
      "  -23.02585   ]\n",
      " [  1.5879908    3.416619     8.806359   ... -23.02585    -23.02585\n",
      "  -23.02585   ]\n",
      " [-21.300102   -12.410904   -15.983486   ... -23.02585    -23.02585\n",
      "  -23.02585   ]]\n",
      "(256000, 200)\n"
     ]
    }
   ],
   "source": [
    "print(predictions)\n",
    "print(predictions.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ltr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
